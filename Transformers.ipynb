{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8QHJVkCcbpl4Dg4hYNu7N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaJ9082/NLP/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Query,Key,Value"
      ],
      "metadata": {
        "id": "aF6D60nmm1nN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S_54b8Kmwlf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l,d_k,d_v=4,8,8#seq length,query and key vector dim\n",
        "q=np.random.randn(l,d_k)\n",
        "k=np.random.randn(l,d_k)\n",
        "v=np.random.randn(l,d_v)\n"
      ],
      "metadata": {
        "id": "UX1XSQrEnFDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q\",q)\n",
        "print(\"K\",k)\n",
        "print(\"V\",v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyYo8NWNnZ1j",
        "outputId": "71f5ad5a-9bfc-495a-e8f7-2d882bcd6e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q [[ 1.78805178 -0.97709767 -0.56062213  0.14574544 -0.46574268  0.07245262\n",
            "   0.28151529 -0.65062424]\n",
            " [ 0.25239774  0.61786864  0.00446839 -1.18138859  1.61240732 -0.42076995\n",
            "  -0.66835692  1.01128964]\n",
            " [-0.10833257 -0.68804893 -0.98784707 -0.4058536   1.618865    0.87893645\n",
            "  -1.34027978  0.04356557]\n",
            " [-0.96836213 -0.19840949  0.21115702 -1.38028163  0.31577939  0.8340221\n",
            "   0.33326597 -0.07538146]]\n",
            "K [[-0.61201758  2.23609807 -0.85985637  0.2453094  -0.59652938 -0.29199138\n",
            "  -1.50659413  0.38821032]\n",
            " [-0.94631208  1.15549986 -0.25802011 -2.24477849  0.16352622 -0.09712039\n",
            "   0.30080652 -0.02394309]\n",
            " [-0.63830042 -0.36017393  0.02671005 -0.2671434   0.4601723  -0.48624236\n",
            "  -1.01478747 -1.04662445]\n",
            " [-0.47462319 -0.75215272 -0.1165112   0.19233326  0.9004559  -1.44158313\n",
            "  -1.61739572 -0.09663339]]\n",
            "V [[-0.74377148 -1.90359234 -1.33417314  1.1418333   1.03203071 -1.86270414\n",
            "   0.52654133  0.96171157]\n",
            " [ 0.12167117 -1.12273009 -0.69987221 -0.08198949 -0.74948342  0.31007397\n",
            "   0.63690156 -1.04189309]\n",
            " [-0.80050503 -1.2498606   0.27664867  1.55961085  0.50870527  0.06034406\n",
            "  -0.78638794  1.89814385]\n",
            " [ 0.8491789   0.70041556  0.46161283  0.53717597  1.22811247  1.65137481\n",
            "  -1.82748711  0.10920047]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Self Attention"
      ],
      "metadata": {
        "id": "00Gl7QpUrcdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "self attention=softmax((q*k.t/(d_k)^1/2)+M)*V"
      ],
      "metadata": {
        "id": "ne7X0I-crg2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.matmul(q,k.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq27Pf_8rgqx",
        "outputId": "3c37d27e-39d1-465f-84b9-4798f2871796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.1814328 , -2.98654384, -0.69756864, -0.93665055],\n",
              "       [ 1.49404368,  3.20517955,  0.49845487,  2.22948349],\n",
              "       [ 0.09143132,  0.24856806,  2.03108471,  2.9601806 ],\n",
              "       [-1.33442856,  3.80374551,  0.54441902, -1.13094023]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var(),k.var(),np.matmul(q,k.T).var()#the variance of multiplication is much higher"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDH7bTIGnhOP",
        "outputId": "8221061a-7422-4b35-a4ea-c7a2ed867e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6850279138706385, 0.7221737108881208, 4.124295087233898)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled=np.matmul(q,k.T)/np.sqrt(d_k)\n",
        "print(\"Scaled\",scaled)\n",
        "print(\"Scaled Variance\",scaled.var())#low var and almost have same range"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRuaYGWxqWcB",
        "outputId": "0ac48584-adc1-474d-d389-d41148aa996e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled [[-1.12480636 -1.0559027  -0.24662776 -0.33115598]\n",
            " [ 0.52822421  1.1332021   0.17623041  0.78824145]\n",
            " [ 0.03232585  0.08788208  0.71809688  1.04658189]\n",
            " [-0.47179174  1.34482712  0.19248119 -0.39984775]]\n",
            "Scaled Variance 0.5155368859042373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Masking"
      ],
      "metadata": {
        "id": "6VMhe_YrtETg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Masking is required to ensure that words dont get context from words generated in the future\n",
        "*  Only required in decoders and not required in encoders\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OiYjDJrvtGfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask=np.tril(np.ones((l,l)))"
      ],
      "metadata": {
        "id": "GxT71WqJsiji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask#my can only look at iself and nothing else. name can only look at my and name and nothing else"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BjRYAASxomU",
        "outputId": "e6032e48-1f85-4da6-ca55-94dab95a3ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask[mask==0]=-np.infty\n",
        "mask[mask==1]=0#we do this because we are adding it to scaled and also we are using softmax function"
      ],
      "metadata": {
        "id": "B9ujzDDyxxdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_U5w5eSyVBB",
        "outputId": "1a2cdb82-4958-44f9-f523-64e61cd758f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled+mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdjKvM8DygZK",
        "outputId": "a8291413-4a47-4e43-9770-adc0babf5da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.12480636,        -inf,        -inf,        -inf],\n",
              "       [ 0.52822421,  1.1332021 ,        -inf,        -inf],\n",
              "       [ 0.03232585,  0.08788208,  0.71809688,        -inf],\n",
              "       [-0.47179174,  1.34482712,  0.19248119, -0.39984775]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Softmax"
      ],
      "metadata": {
        "id": "KpiMTsJhy8UV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values would all be added up to one and also be interpretible and stable and"
      ],
      "metadata": {
        "id": "rVi8sXvGzAbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T"
      ],
      "metadata": {
        "id": "kp0G7YqiyrEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention=softmax(scaled+mask)"
      ],
      "metadata": {
        "id": "SjKP9kwxzmfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention#my will only focis on my. name will only focus on my and name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7M-yuAdzvn-",
        "outputId": "4e795af5-50a5-468f-eccf-3037be4a3984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.35320566, 0.64679434, 0.        , 0.        ],\n",
              "       [0.24737593, 0.26150814, 0.49111593, 0.        ],\n",
              "       [0.098341  , 0.60489806, 0.19108417, 0.10567677]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_v=np.matmul(attention,v)"
      ],
      "metadata": {
        "id": "JAXlRlfQzwVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_v#capture the better context of words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE4MC27i0I9s",
        "outputId": "dd41d180-6481-44e4-b1a3-6df7ba9aee57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.74377148, -1.90359234, -1.33417314,  1.1418333 ,  1.03203071,\n",
              "        -1.86270414,  0.52654133,  0.96171157],\n",
              "       [-0.18400808, -1.39853506, -0.92391089,  0.35027164, -0.12024255,\n",
              "        -0.45736355,  0.5979217 , -0.33420859],\n",
              "       [-0.54531394, -1.37833243, -0.37719803,  1.02697089,  0.30913681,\n",
              "        -0.35006538, -0.08939905,  0.89764945],\n",
              "       [-0.06276994, -1.03114936, -0.45291033,  0.41747771, -0.12488165,\n",
              "         0.19042569,  0.0936519 , -0.16141823]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v#the first vector of old and new vector that is vector for my word will be same"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEBmuLRH0J-q",
        "outputId": "b3aabcf3-0176-494c-ac6e-1674c5d4c176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.74377148, -1.90359234, -1.33417314,  1.1418333 ,  1.03203071,\n",
              "        -1.86270414,  0.52654133,  0.96171157],\n",
              "       [ 0.12167117, -1.12273009, -0.69987221, -0.08198949, -0.74948342,\n",
              "         0.31007397,  0.63690156, -1.04189309],\n",
              "       [-0.80050503, -1.2498606 ,  0.27664867,  1.55961085,  0.50870527,\n",
              "         0.06034406, -0.78638794,  1.89814385],\n",
              "       [ 0.8491789 ,  0.70041556,  0.46161283,  0.53717597,  1.22811247,\n",
              "         1.65137481, -1.82748711,  0.10920047]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting everything in one function"
      ],
      "metadata": {
        "id": "opRYNHbT1B-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T\n",
        "\n",
        "def scaled_attention(q,k,v,mask=None):\n",
        "  d_k=q.shape[-1]\n",
        "  scaled=np.matmul(q,k.T)/math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled=scaled+mask\n",
        "  attention=softmax(scaled)\n",
        "  output=np.matmul(attention,v)\n",
        "  return output,attention"
      ],
      "metadata": {
        "id": "L1LmEFji0hPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values,attention=scaled_attention(q,k,v,mask=None)#mask is none for encoder and not none for decoder\n",
        "print(\"Q\\n\",q)\n",
        "print(\"K\\n\",k)\n",
        "print(\"Old V\\n\",v)\n",
        "print(\"New V\\n\",values)\n",
        "print(\"Attention\\n\",attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quEd4Syq19dQ",
        "outputId": "d4ae19cb-d83f-4e3c-dd96-0f812e9acc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[ 1.78805178 -0.97709767 -0.56062213  0.14574544 -0.46574268  0.07245262\n",
            "   0.28151529 -0.65062424]\n",
            " [ 0.25239774  0.61786864  0.00446839 -1.18138859  1.61240732 -0.42076995\n",
            "  -0.66835692  1.01128964]\n",
            " [-0.10833257 -0.68804893 -0.98784707 -0.4058536   1.618865    0.87893645\n",
            "  -1.34027978  0.04356557]\n",
            " [-0.96836213 -0.19840949  0.21115702 -1.38028163  0.31577939  0.8340221\n",
            "   0.33326597 -0.07538146]]\n",
            "K\n",
            " [[-0.61201758  2.23609807 -0.85985637  0.2453094  -0.59652938 -0.29199138\n",
            "  -1.50659413  0.38821032]\n",
            " [-0.94631208  1.15549986 -0.25802011 -2.24477849  0.16352622 -0.09712039\n",
            "   0.30080652 -0.02394309]\n",
            " [-0.63830042 -0.36017393  0.02671005 -0.2671434   0.4601723  -0.48624236\n",
            "  -1.01478747 -1.04662445]\n",
            " [-0.47462319 -0.75215272 -0.1165112   0.19233326  0.9004559  -1.44158313\n",
            "  -1.61739572 -0.09663339]]\n",
            "Old V\n",
            " [[-0.74377148 -1.90359234 -1.33417314  1.1418333   1.03203071 -1.86270414\n",
            "   0.52654133  0.96171157]\n",
            " [ 0.12167117 -1.12273009 -0.69987221 -0.08198949 -0.74948342  0.31007397\n",
            "   0.63690156 -1.04189309]\n",
            " [-0.80050503 -1.2498606   0.27664867  1.55961085  0.50870527  0.06034406\n",
            "  -0.78638794  1.89814385]\n",
            " [ 0.8491789   0.70041556  0.46161283  0.53717597  1.22811247  1.65137481\n",
            "  -1.82748711  0.10920047]]\n",
            "New V\n",
            " [[-0.09895298 -0.68247475 -0.05940459  0.89623108  0.62326473  0.33884668\n",
            "  -0.70634923  0.69587191]\n",
            " [ 0.00360059 -0.81345183 -0.37722324  0.5764803   0.33326151  0.184065\n",
            "  -0.25465972  0.1097699 ]\n",
            " [ 0.02015619 -0.53539523 -0.03705825  0.8283579   0.6817836   0.46152333\n",
            "  -0.79419786  0.57793153]\n",
            " [-0.06276994 -1.03114936 -0.45291033  0.41747771 -0.12488165  0.19042569\n",
            "   0.0936519  -0.16141823]]\n",
            "Attention\n",
            " [[0.14949248 0.16015622 0.35975551 0.33059579]\n",
            " [0.20697726 0.37901918 0.1455639  0.26843966]\n",
            " [0.14706441 0.15546597 0.29196726 0.40550236]\n",
            " [0.098341   0.60489806 0.19108417 0.10567677]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tybIJ5hn2Dyo",
        "outputId": "269e8d75-805c-4c59-8cfe-d182f31ae69a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.09895298, -0.68247475, -0.05940459,  0.89623108,  0.62326473,\n",
              "         0.33884668, -0.70634923,  0.69587191],\n",
              "       [ 0.00360059, -0.81345183, -0.37722324,  0.5764803 ,  0.33326151,\n",
              "         0.184065  , -0.25465972,  0.1097699 ],\n",
              "       [ 0.02015619, -0.53539523, -0.03705825,  0.8283579 ,  0.6817836 ,\n",
              "         0.46152333, -0.79419786,  0.57793153],\n",
              "       [-0.06276994, -1.03114936, -0.45291033,  0.41747771, -0.12488165,\n",
              "         0.19042569,  0.0936519 , -0.16141823]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l=[1,2,3]\n",
        "print(*l)"
      ],
      "metadata": {
        "id": "Y-IQZ-Q42ExO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6686be25-d32f-45b3-f441-1973303a201e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder"
      ],
      "metadata": {
        "id": "UCWz7XqZe_qG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "U9PH5Mwir7zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# x=np.zeros(shape=(30,200,512))\n"
      ],
      "metadata": {
        "id": "J_962oNvCI7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_attention_prod(q,k,v,mask=None):\n",
        "  #q,k,v=30,8,200,64\n",
        "  d_k=q.size()[-1]#64\n",
        "  scaled=F.matmul(q,k.transpose(-1,-2))/np.sqrt(d_k)#k_size=batch_size,msl,emb after transpose->batch_size,emb,msl---->30,8,200,200\n",
        "  if mask is not None:\n",
        "    scaled=scaled+mask#broadcasting since shape of scaled is 30,8,200,200 and mask is 200,200\n",
        "  attention=F.softmax(scaled,dim=-1)# 30,8,200,200 -> also adding only values so dim=-1\n",
        "  output=F.matmul(attention,v)#30,8,200,64 -> 64 dimensional embedding\n",
        "  return attention,output"
      ],
      "metadata": {
        "id": "JJXwjbI8-6KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=[-(i+1) for i in range(len([512]))]\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvapXGeRTYVd",
        "outputId": "7ea9c242-612c-4c85-c5dc-4724afebcc3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model#512\n",
        "    self.num_heads=num_heads#8\n",
        "    self.head_dim=d_model//num_heads#64\n",
        "    self.qkv_layer=nn.Linear(d_model,d_model*3)#they are not individual q,kv but they are stacked tensors 512*1536\n",
        "    self.linear_layer=nn.Linear(d_model,d_model)#512*512\n",
        "\n",
        "  def forward(self,x):\n",
        "    batch_size,m_s_l,d_model=x.size()#30,200,512\n",
        "    qkv=self.qkv_layer(x)#30,200,1536\n",
        "    qkv=qkv.reshape(batch_size,m_s_l,self.num_heads,3*self.head_dim)#30,200,8,192 ->dividing the 1536 into 8 heads each having 64 dims for q,k,v respectively\n",
        "    qkv=qkv.permute(0,2,1,3)#30,8,200,192\n",
        "    q,k,v=qkv.chunk(3,dim=-1)#each 30,8,200,64\n",
        "    values,attention=scaled_attention_prod(q,k,v,mask=None)#attention 30,8,200,200  value 30,8,200,64\n",
        "    values=values.reshape(batch_size,m_s_l,self.head*self.head_dim)#30,200,512\n",
        "    out=self.linear_layer(values)\n",
        "    return out\n",
        "\n",
        "class LayerNormalisation(nn.Module):\n",
        "  def __init__(self,parameters_shape,eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameters_shape=parameters_shape#[512]\n",
        "    self.eps=eps\n",
        "    self.gamma=nn.Parameter(torch.ones(parameters_shape))#learnable parameter [512] -> 512 dimesionals tensors\n",
        "    self.beta=nn.Parameter(torch.ones(parameters_shape))\n",
        "\n",
        "  def forward(self,inputs):#30,200,512\n",
        "    dims=[-(i+1) for i in range(len(self.parameters_shape))]#[-1]\n",
        "    mean=inputs.mean(dim=dims,keepdim=True)# 30,200,1 taking mean along last dimension that is embedding dimension for eg we a embedding of a word we take the mean of that whole embedding vector\n",
        "    var=((inputs-mean)**2).mean(dim=dims,keepdim=True)#30,200,1\n",
        "    std=(var+self.eps).sqrt()#30,200,1\n",
        "    y=(inputs-mean)/std# 30,200,512 for every number in last dimension we subtract by mean and divide by std\n",
        "    out=self.gamma*y+self.beta#30,200,512\n",
        "    return out\n",
        "\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,drop_prob):\n",
        "    super(PositionWiseFeedForward,self).__init__()\n",
        "    self.linear1=nn.Linear(d_model,ffn_hidden)#512,2048\n",
        "    self.linear2=nn.Linear(ffn_hidden,d_model)#2048,512\n",
        "    self.relu=nn.ReLu()\n",
        "    self.dropout=nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self,x):#30,200,512\n",
        "    x=self.linear1(x)#30,200,2048\n",
        "    x=self.relu(x)#30,200,512\n",
        "    x=self.dropout(x)#30,200,512\n",
        "    x=self.linear2(x)#30,200,512\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rsyDTlru6hf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model=512\n",
        "n_heads=8\n",
        "drop_prob=0.1\n",
        "batch_size=30\n",
        "m_s_l=200\n",
        "ffn_hidden=2048\n",
        "num_layers=5\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers):\n",
        "    super().__init__()\n",
        "    self.layers=nn.Sequential(*[EncoderLayer(d_model,ffn_hidden,num_heads,drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.layers(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "PcmKoL8bfCJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
        "    super(EncoderLayer,self).__init__()\n",
        "    self.attention=MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
        "    self.norm1=LayerNormalisation(parameters_shape=[d_model])\n",
        "    self.dropout1=nn.Dropout(p=drop_prob)\n",
        "    self.ffn=PositionWiseFeedForward(d_model=d_model,hidden=ffn_hidden,drop_prob=drop_prob)\n",
        "    self.norm2=LayerNormalisation(parameters_shape=[d_model])\n",
        "    self.dropout2=nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self,x):\n",
        "    residual_conn=x#30,200,512\n",
        "    x=self.attention(x,mask=None)#30,200,512\n",
        "    x=self.dropout1(x)#30,200,512\n",
        "    x=self.norm1(residual_conn+x)#30,200,512\n",
        "    residual_x=x#30,200,512\n",
        "    x=self.ffn(x)#30,200,512\n",
        "    x=self.dropout2(x)#30,200,512\n",
        "    x=self.norm2(x+residual_conn)#30,200,512\n",
        "    return x#much more context aware\n"
      ],
      "metadata": {
        "id": "Lj-vFNUByWVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decoder"
      ],
      "metadata": {
        "id": "pLi2khu2ra5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "aPhso03csYa0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q,k,v,mask=None):\n",
        "  #q=30,8,200,64 mask=200,200\n",
        "  d_k=q.size()[-1]#64\n",
        "  scaled=F.matmul(q,k.transpose(-1,-2))/np.sqrt(d_k)#k_size=batch_size,msl,emb after transpose->batch_size,emb,msl---->30,8,200,200\n",
        "  print('scaled.size()',scaled.size())\n",
        "  if mask is not None:\n",
        "    print('mask size',mask.size())\n",
        "    scaled=scaled+mask#broadcasting since shape of scaled is 30,8,200,200 and mask is 200,200\n",
        "  attention=F.softmax(scaled,dim=-1)# 30,8,200,200 -> also adding only values so dim=-1  how much atention one word pays to every other words\n",
        "  output=F.matmul(attention,v)#30,8,200,64 -> 64 dimensional embedding   context aware embeddings\n",
        "  return attention,output\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.num_heads=num_heads\n",
        "    self.head_dim=d_model//num_heads\n",
        "    self.qkv_layer=nn.Linear(d_model,d_model*3)\n",
        "    self.linear_layer=nn.Linear(d_model,d_model)\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "    batch_size,m_s_l,d_model=x.size()#30,200,512\n",
        "    print(\"x.size()\",x.size())\n",
        "    qkv=self.qkv_layer(x)#30,200,1536\n",
        "    print('qkv.size',qkv.size())\n",
        "    qkv=qkv.reshape(batch_size,m_s_l,self.num_heads,self.head_dim*3)#30,200,8,192\n",
        "    print(\"qkv after reshape\",qkv.size())\n",
        "    qkv=qkv.permute(0,2,1,3)#30,8,200,192\n",
        "    print(\"qkv after permutation\",qkv.size())\n",
        "    q,k,v=qkv.chunk(3,dim=-1)# each 30,8,200,64\n",
        "    print(\"q size\",q.size(),\"k size\",k.size(),\"v size\",v.size())\n",
        "    values,attention=scaled_dot_product(q,k,v,mask)#values=30,8,200,64\n",
        "    print('values.size()' ,values.size(),'attention.size()',attention.size())\n",
        "    values=values.reshape(batch_size,m_s_l,self.num_heads*self.head_dim)#30,200,512\n",
        "    print('values after reshaping',values.size())\n",
        "    out=self.linear_layer(values)\n",
        "    print('output after passing thorugh linear layer',out.size())\n",
        "    return out\n",
        "\n",
        "class LayerNormalisation(nn.Module):\n",
        "  def __init__(self,parameters_shape,eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameters_shape=parameters_shape#[512]\n",
        "    self.eps=eps\n",
        "    self.gamma=nn.Parameter(torch.ones(parameters_shape))#learnable parameter [512] -> 512 dimesionals tensors\n",
        "    self.beta=nn.Parameter(torch.ones(parameters_shape))#512\n",
        "\n",
        "  def forward(self,inputs):#30,200,512\n",
        "    dims=[-(i+1) for i in range(len(self.parameters_shape))]#[-1]\n",
        "    #if keepdim=False output of mean is #30,200 ,if keepdim=True op is #20,200,1\n",
        "    mean=inputs.mean(dim=dims,keepdim=True) # 30,200,1 taking mean along last dimension that is embedding dimension for eg we a embedding of a word we take the mean of that whole embedding vector\n",
        "    var=((inputs-mean)**2).mean(dim=dims,keepdim=True)#30,200,1\n",
        "    std=(var+self.eps).sqrt()#30,200,1\n",
        "    y=(inputs-mean)/std# 30,200,512 for every number in last dimension we subtract by mean and divide by std\n",
        "    out=self.gamma*y+self.beta#30,200,512\n",
        "    return out\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.num_heads=num_heads\n",
        "    self.head_dim=d_model//num_heads\n",
        "    self.kv_layer=nn.Linear(d_model,d_model*2)\n",
        "    self.q_layer=nn.Linear(d_model,d_model)\n",
        "    self.linear_layer=nn.Linear(d_model,d_model)\n",
        "\n",
        "  def forward(self,x,y,mask=None):\n",
        "    batch_size,m_s_l,d_model=x.size()\n",
        "    q=self.q_layer(y)#30,200,512\n",
        "    print('q.size()',q.size())\n",
        "    q=q.reshape(batch_size,m_s_l,self.num_heads,self.head_dims)#30,200,8,64\n",
        "    kv=self.kv_layer(x)#30,200,1024\n",
        "    print('kv.size',kv.size())\n",
        "    kv=kv.reshape(batch_size,m_s_l,self.num_heads,2*self.head_dims)#30,200,8,128\n",
        "    kv=kv.permute(0,2,1,3)#30,8,200,64\n",
        "    q=q.permute(0,2,1,3)#30,8,200,64\n",
        "\n",
        "    k,v=kv.chunk(2,dim=-1)#each 30,8,200,64\n",
        "    values,attention=scaled_dot_product(q,k,v,mask)\n",
        "    print('values.size',values.size,'attention.size()',attention.size())\n",
        "    values=values.reshape(batch_size,m_s_l,self.num_heads*self.head_dim)\n",
        "    print('value after reshaping',values.size())\n",
        "    out=self.linear_layer(values)#30,200,512\n",
        "    print('op after passing through linear layer',out.size())\n",
        "    return out\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,drop_prob):\n",
        "    super(PositionWiseFeedForward,self).__init__()\n",
        "    self.linear1=nn.Linear(d_model,ffn_hidden)#512,2048\n",
        "    self.linear2=nn.Linear(ffn_hidden,d_model)#2048,512\n",
        "    self.relu=nn.ReLu()\n",
        "    self.dropout=nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self,x):#30,200,512\n",
        "    x=self.linear1(x)#30,200,2048\n",
        "    print('shape after 1st linear layer',x.size())\n",
        "    x=self.relu(x)#30,200,2048\n",
        "    print('shape after relu',x.size())\n",
        "    x=self.dropout(x)#30,200,2048\n",
        "    print('shape after dropout',x.size())\n",
        "    x=self.linear2(x)#30,200,512\n",
        "    print('shape after 2nd linear layer',x.size())\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
        "    super(DecoderLayer).__init__()\n",
        "    self.self_attention=MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
        "    self.layernorm1=LayerNormalisation(parameters_shape=[d_model])\n",
        "    self.dropout1=nn.Dropout(p=drop_prob)\n",
        "    self.multiheadcrossattention=MultiHeadCrossAttention(d_model=d_model,num_heads=num_heads)\n",
        "    self.layernorm2=LayerNormalisation(parameters_shape=[d_model])\n",
        "    self.dropout2=nn.Dropout(p=drop_prob)\n",
        "    self.feedforward=FeedForwardNetwork(d_model=d_model,ffn_hidden=ffn_hidden,drop_prob=drop_prob)\n",
        "    self.layernorm3=LayerNormalisation(parameters_shape=[d_model])\n",
        "    self.dropout3=nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self,x,y,decoder_mask):\n",
        "    residual_conn=y\n",
        "    print('Maked self attention')\n",
        "    y=self.self_attention(y,mask=decoder_mask)#30,200,512\n",
        "    print('Dropout')\n",
        "    y=self.dropout1(y)#30,200,512\n",
        "    print('Layer normalisation')\n",
        "    y=self.layernorm1(y+residual_conn)#30,200,512\n",
        "    residual_conn=y\n",
        "    print('Cross Attention')\n",
        "    y=self.multiheadcrossattention(x,y,mask=None)#30,200,512\n",
        "    print('Dropout')\n",
        "    y=self.dropout2(y)#30,200,512\n",
        "    print('Layer normalisation')\n",
        "    y=self.layernorm2(y+residual_conn)#30,200,512\n",
        "    residual_conn=y#30,200,512\n",
        "    print('Feed Forward')\n",
        "    y=self.feedforward(y)\n",
        "\n",
        "\n",
        "class SequentialDecoder(nn.Sequential):\n",
        "  def forward(self,*inputs):\n",
        "    x,y,mask=inputs\n",
        "    for module in self._modules.values():\n",
        "      y=module(x,y,mask)#produces output of one decoder layer after another\n",
        "    return y\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers=1):\n",
        "    super().__init__():\n",
        "    self.layers=SequentialDecoder(*[DecoderLayer(d_model,ffn_hidden,num_heads,d_prob)for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self,x,y,mask):#if only use sequential then we cannot pass more than one value\n",
        "    #x=30,200,512 y=#30,200,512 #mask=200,200\n",
        "    y=self.layers(x,y,mask)\n",
        "    return y\n",
        "\n"
      ],
      "metadata": {
        "id": "yd41JZFvyr8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model=512\n",
        "d_prob=0.1\n",
        "m_s_l=200\n",
        "ffn_hidden=2048\n",
        "batch_size=30\n",
        "num_layers=5\n",
        "num_heads=8\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#   def __init__():\n",
        "\n",
        "x=torch.randn((batch_size,m_s_l,d_model))#english sentence position encoded\n",
        "y=torch.randn((batch_size,m_s_l,d_model))#kannada sentence position encoded\n",
        "mask=torch.full([m_s_l,m_s_l],float('-inf'))\n",
        "mask=torch.triu(mask,diagonal=1)\n",
        "decoder=Decoder(d_model,ffn_hidden,num_heads,drop_prob,num_layers)\n",
        "out=decoder(x,y,mask)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYAQtrDgrcMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}